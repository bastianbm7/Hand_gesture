{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3466658-3356-4126-b403-e44d4d57cc0d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Consulta a ChatGPT usando tu mano y tu micrófono"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922c2d86-3ae5-4ca8-adc8-1c2f01bd3310",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Al correr el código, se abrirá una la cámara y podrás cambiar distintas funciones:**\n",
    "\n",
    "1) Cambia el valor de 'reconocer_manos' para que reconocer las manos en el video.\n",
    "\n",
    "2) Cambia el valor de 'usar_numeros' para mostrar las 21 marcas en la mano.\n",
    "\n",
    "3) Usa 'cambiar_color' para cambiar el color de algunas marcas de la mano. Se debe ingresar como lista con valores entre 0 a 20, por ejemplo; \\[2, 5, 8, 17\\]\n",
    "\n",
    "4) Cambia 'color' para cambiar el color de las marcas señaladas. Los valores son en escala BGR (Azul, Verde, Rojo) desde 0 a 255.\n",
    "\n",
    "5) Usa 'pregunta' para realizar una pregunta a chatGPT cuando tengas las manos cerca de la pantalla. Una vez realizada la pregunta, vuelve a alejarte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ea75fd3-7835-4f0f-9190-c62c50850607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------\n",
    "# Read libraries\n",
    "# ----------------------------------------------\n",
    "import cv2\n",
    "import time\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import openai\n",
    "import speech_recognition as sr\n",
    "import mediapipe as mp\n",
    "import pyttsx3\n",
    "\n",
    "from ultralytics import YOLO\n",
    "from ultralytics.models.yolo.detect.predict import DetectionPredictor\n",
    "\n",
    "import Codes\n",
    "\n",
    "# ----------------------------------------------\n",
    "# Inicializadores\n",
    "# ----------------------------------------------\n",
    "\n",
    "# Configura tu clave de API de OpenAI aquí\n",
    "# openai.api_key = \"YOUR API KEY\"\n",
    "# Inicializa el reconocedor de voz\n",
    "recognizer = sr.Recognizer()\n",
    "# Initialize MediaPipe Hands\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands()\n",
    "# Initialize MediaPipe Drawing\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Inicializar el sintetizador de voz\n",
    "engine = pyttsx3.init()\n",
    "engine.setProperty('rate', 180)\n",
    "\n",
    "# ----------------------------------------------\n",
    "# Hand recognizer implemented with chat GPT\n",
    "# ----------------------------------------------\n",
    "def handTracking(show, number, changeColor, color, question):\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            continue\n",
    "        # Convert the BGR image to RGB\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        # Process the frame to detect hands\n",
    "        results = hands.process(frame_rgb)\n",
    "    \n",
    "        if results.multi_hand_landmarks:\n",
    "            # Determinar el centro de la imagen\n",
    "            center_x = frame.shape[1] // 2\n",
    "            \n",
    "            for i, landmarks in enumerate(results.multi_hand_landmarks):\n",
    "                # Calcular la posición horizontal del centro de la mano\n",
    "                x_min = min(landmark.x for landmark in landmarks.landmark)\n",
    "                x_max = max(landmark.x for landmark in landmarks.landmark)\n",
    "                hand_center_x = (x_min + x_max) / 2 * frame.shape[1]\n",
    "                \n",
    "                # Determinar la mano izquierda o derecha\n",
    "                if hand_center_x < center_x:\n",
    "                    hand_type = \"Mano derecha\"\n",
    "                else:\n",
    "                    hand_type = \"Mano Izquierda\"\n",
    "                \n",
    "                # Draw landmarks on the frame\n",
    "                mp_drawing.draw_landmarks(frame, landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "    \n",
    "                # Obtener las coordenadas del bounding box\n",
    "                x_min = min(landmark.x for landmark in landmarks.landmark)\n",
    "                y_min = min(landmark.y for landmark in landmarks.landmark)\n",
    "                x_max = max(landmark.x for landmark in landmarks.landmark)\n",
    "                y_max = max(landmark.y for landmark in landmarks.landmark)\n",
    "                \n",
    "                # Dibujar el cuadro alrededor de la mano\n",
    "                cv2.rectangle(frame, (int(x_min * frame.shape[1]), int(y_min * frame.shape[0])),\n",
    "                              (int(x_max * frame.shape[1]), int(y_max * frame.shape[0])),\n",
    "                              (0, 255, 0), 2)\n",
    "                \n",
    "                # Agregar texto con el tipo de mano\n",
    "                cv2.putText(frame, hand_type, (int(x_min * frame.shape[1]), int(y_min * frame.shape[0]) - 10),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)\n",
    "    \n",
    "                #$if question:\n",
    "                #$    # Calcular el área del rectángulo que contiene la mano\n",
    "                #$    area = (x_max - x_min) * (y_max - y_min) * 1000\n",
    "                #$\n",
    "                #$    # Si el área del rectángulo es mayor que un valor umbral, reconoce el discurso\n",
    "                #$    if area > 170:  # Ajusta este valor según tu necesidad\n",
    "                #$        with sr.Microphone() as source:\n",
    "                #$            # Indica las instrucciones con el sintetizador de voz\n",
    "                #$            engine.say('Has activado Chat GPT. Realiza una breve pregunta')\n",
    "                #$            engine.runAndWait()\n",
    "                #$            \n",
    "                #$            # Reconocer voz a texto\n",
    "                #$            recognizer.adjust_for_ambient_noise(source)\n",
    "                #$            audio = recognizer.listen(source)\n",
    "                #$            \n",
    "                #$            # Indica instrucciones con sintetizador de voz\n",
    "                #$            engine.say('Procesando tu pregunta')\n",
    "                #$            engine.runAndWait()\n",
    "                #$        \n",
    "                #$        # Intenta entender la pregunta\n",
    "                #$        try:\n",
    "                #$            pregunta = recognizer.recognize_google(audio, language=\"es-ES\")\n",
    "                #$        except sr.UnknownValueError:\n",
    "                #$            print(\"No se pudo entender el audio\")\n",
    "                #$        except sr.RequestError as e:\n",
    "                #$            print(\"Error en la solicitud: {0}\".format(e))\n",
    "                #$        \n",
    "                #$        # Si se entiende la pregunta, se ingresa como input a chatGPT\n",
    "                #$        if pregunta:\n",
    "                #$            respuesta = openai.Completion.create(\n",
    "                #$                engine=\"text-davinci-002\",\n",
    "                #$                prompt=pregunta,\n",
    "                #$                max_tokens=200  # Ajusta el número de tokens según tu preferencia\n",
    "                #$            )\n",
    "                #$            print(\"Pregunta:\", pregunta)\n",
    "                #$            \n",
    "                #$            # Guardar respuesta\n",
    "                #$            respuesta_texto = respuesta.choices[0].text\n",
    "                #$            \n",
    "                #$            # Usamos el sintetizador de voz para reproducir la respuesta\n",
    "                #$            engine.say(respuesta_texto)\n",
    "                #$            engine.runAndWait()\n",
    "                #$            time.sleep(1)\n",
    "                #$            \n",
    "                #$            # Realiza instrucciones con el sintetizador de voz\n",
    "                #$            engine.say('Gracias. Ahora puedes volver a alejar tu mano')\n",
    "                #$            engine.runAndWait()\n",
    "                #$            time.sleep(1)\n",
    "                    \n",
    "                if number == True:\n",
    "                    # Add numbers to landmarks\n",
    "                    for idx, landmark in enumerate(landmarks.landmark):\n",
    "                        h, w, _ = frame.shape\n",
    "                        cx, cy = int(landmark.x * w), int(landmark.y * h)\n",
    "                        cv2.putText(frame, str(idx), (cx, cy), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n",
    "                \n",
    "                if changeColor is not None:\n",
    "                    for mark in changeColor:\n",
    "                        # Change the color of a specific landmark (e.g., landmark 8)\n",
    "                        landmark_index_to_color = mark\n",
    "                        landmark_to_change = landmarks.landmark[landmark_index_to_color]\n",
    "                        cx, cy = int(landmark_to_change.x * frame.shape[1]), int(landmark_to_change.y * frame.shape[0])\n",
    "                        cv2.circle(frame, (cx, cy), 5, color, -1)  # Change color to red (BGR format)\n",
    "        \n",
    "        # Write a instructions on screen\n",
    "        #if question:\n",
    "        #    cv2.putText(frame, f'Acerca tu mano para realizar una consulta a chatGPT',\n",
    "        #                (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 150, 255), 2)\n",
    "        \n",
    "        cv2.putText(frame, f'Saluda! Te estamos grabando :)',\n",
    "                    (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (50, 50, 255), 3)\n",
    "        \n",
    "        width, height = 1600, 900\n",
    "        frame = cv2.resize(frame, (width, height))\n",
    "        cv2.imshow('Hand Landmarks', frame)\n",
    "        \n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# ----------------------------------------------\n",
    "# Use sign language recognizer from roboflow\n",
    "# ----------------------------------------------\n",
    "def implementation(model_directory, labels, source):\n",
    "    model = YOLO(model_directory)\n",
    "\n",
    "    model.predict(source=source, show=True)  # Generador de objetos Results\n",
    "    results = model(source=source, stream=True, show = True)  # Generador de objetos Results\n",
    "    \n",
    "    for r in results:\n",
    "        boxes = r.boxes  # Objetos Boxes para las salidas de cuadros delimitadores\n",
    "        masks = r.masks  # Objetos Masks para las salidas de máscaras de segmentación\n",
    "        probs = r.probs  # Probabilidades de clase para las salidas de clasificación\n",
    "        \n",
    "        count = 0\n",
    "        if len(boxes) > 0:\n",
    "            classes = int(boxes.cls[0])\n",
    "            predCharacter = labels[classes]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8cdddd-9ad3-47c8-998c-753c3046c8f2",
   "metadata": {},
   "source": [
    "## Usage: Hand detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5637281a-e610-4009-a3ef-186c47fe49a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Open the webcam\n",
    "cap = cv2.VideoCapture(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5a909de-5709-4091-94e2-1a7ea2394b96",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.8.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\highgui\\src\\window.cpp:1272: error: (-2:Unspecified error) The function is not implemented. Rebuild the library with Windows, GTK+ 2.x or Cocoa support. If you are on Ubuntu or Debian, install libgtk2.0-dev and pkg-config, then re-run cmake or configure script in function 'cvShowImage'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m color \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m250\u001b[39m, \u001b[38;5;241m155\u001b[39m, \u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# Colores BGR (Blue, Green, Red)\u001b[39;00m\n\u001b[0;32m      5\u001b[0m question \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# True / False\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[43mhandTracking\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumber\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchangeColor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[5], line 157\u001b[0m, in \u001b[0;36mhandTracking\u001b[1;34m(show, number, changeColor, color, question)\u001b[0m\n\u001b[0;32m    155\u001b[0m width, height \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1600\u001b[39m, \u001b[38;5;241m900\u001b[39m\n\u001b[0;32m    156\u001b[0m frame \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mresize(frame, (width, height))\n\u001b[1;32m--> 157\u001b[0m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimshow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mHand Landmarks\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cv2\u001b[38;5;241m.\u001b[39mwaitKey(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m&\u001b[39m \u001b[38;5;241m0xFF\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mord\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mq\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    160\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Hands\\lib\\site-packages\\ultralytics\\utils\\patches.py:55\u001b[0m, in \u001b[0;36mimshow\u001b[1;34m(winname, mat)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mimshow\u001b[39m(winname: \u001b[38;5;28mstr\u001b[39m, mat: np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[0;32m     48\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;124;03m    Displays an image in the specified window.\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;124;03m        mat (np.ndarray): Image to be shown.\u001b[39;00m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 55\u001b[0m     \u001b[43m_imshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwinname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43municode_escape\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmat\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.8.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\highgui\\src\\window.cpp:1272: error: (-2:Unspecified error) The function is not implemented. Rebuild the library with Windows, GTK+ 2.x or Cocoa support. If you are on Ubuntu or Debian, install libgtk2.0-dev and pkg-config, then re-run cmake or configure script in function 'cvShowImage'\n"
     ]
    }
   ],
   "source": [
    "show = True  # True / False\n",
    "number = True  # True / False\n",
    "changeColor = [8,17, 4, 16] # Lista con números del 0 al 20.\n",
    "color = (250, 155, 0)  # Colores BGR (Blue, Green, Red)\n",
    "question = False  # True / False\n",
    "\n",
    "handTracking(show, number, changeColor, color, question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860b57aa-9bd1-42e5-a1ab-2626e9074969",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1ba8d94e-181a-4d03-8ce9-5140e5599e37",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Usage: Hand Gesture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "125db4b1-97a6-48f9-b75b-1540006dbede",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ⚠️ Environment does not support cv2.imshow() or PIL Image.show()\n",
      "OpenCV(4.8.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\highgui\\src\\window.cpp:1272: error: (-2:Unspecified error) The function is not implemented. Rebuild the library with Windows, GTK+ 2.x or Cocoa support. If you are on Ubuntu or Debian, install libgtk2.0-dev and pkg-config, then re-run cmake or configure script in function 'cvShowImage'\n",
      "\n",
      "WARNING ⚠️ Video stream unresponsive, please check your IP camera connection.\n",
      "WARNING ⚠️ Video stream unresponsive, please check your IP camera connection.\n",
      "\n"
     ]
    },
    {
     "ename": "ConnectionError",
     "evalue": "1/1: 0... Failed to open 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m labels \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m0\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m1\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mB\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m2\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m3\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m4\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mF\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m5\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRock\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m6\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mU\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m7\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mV\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m8\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mY\u001b[39m\u001b[38;5;124m'\u001b[39m}\n\u001b[0;32m      3\u001b[0m source \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 4\u001b[0m \u001b[43mimplementation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_directory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msource\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[1], line 171\u001b[0m, in \u001b[0;36mimplementation\u001b[1;34m(model_directory, labels, source)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mimplementation\u001b[39m(model_directory, labels, source):\n\u001b[0;32m    169\u001b[0m     model \u001b[38;5;241m=\u001b[39m YOLO(model_directory)\n\u001b[1;32m--> 171\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Generador de objetos Results\u001b[39;00m\n\u001b[0;32m    172\u001b[0m     results \u001b[38;5;241m=\u001b[39m model(source\u001b[38;5;241m=\u001b[39msource, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, show \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)  \u001b[38;5;66;03m# Generador de objetos Results\u001b[39;00m\n\u001b[0;32m    174\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m results:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\hand_\\lib\\site-packages\\ultralytics\\engine\\model.py:239\u001b[0m, in \u001b[0;36mModel.predict\u001b[1;34m(self, source, stream, predictor, **kwargs)\u001b[0m\n\u001b[0;32m    237\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prompts \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mset_prompts\u001b[39m\u001b[38;5;124m'\u001b[39m):  \u001b[38;5;66;03m# for SAM-type models\u001b[39;00m\n\u001b[0;32m    238\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mset_prompts(prompts)\n\u001b[1;32m--> 239\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mpredict_cli(source\u001b[38;5;241m=\u001b[39msource) \u001b[38;5;28;01mif\u001b[39;00m is_cli \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredictor\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\hand_\\lib\\site-packages\\ultralytics\\engine\\predictor.py:198\u001b[0m, in \u001b[0;36mBasePredictor.__call__\u001b[1;34m(self, source, model, stream, *args, **kwargs)\u001b[0m\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream_inference(source, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 198\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\hand_\\lib\\site-packages\\torch\\utils\\_contextlib.py:35\u001b[0m, in \u001b[0;36m_wrap_generator.<locals>.generator_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;66;03m# Issuing `None` to a generator fires it up\u001b[39;00m\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m---> 35\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m     38\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     39\u001b[0m             \u001b[38;5;66;03m# Forward the response to our caller and get its next request\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\hand_\\lib\\site-packages\\ultralytics\\engine\\predictor.py:240\u001b[0m, in \u001b[0;36mBasePredictor.stream_inference\u001b[1;34m(self, source, model, *args, **kwargs)\u001b[0m\n\u001b[0;32m    236\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msetup_model(model)\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:  \u001b[38;5;66;03m# for thread-safe inference\u001b[39;00m\n\u001b[0;32m    239\u001b[0m     \u001b[38;5;66;03m# Setup source every time predict is called\u001b[39;00m\n\u001b[1;32m--> 240\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup_source\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msource\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msource\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    242\u001b[0m     \u001b[38;5;66;03m# Check if save_dir/ label file exists\u001b[39;00m\n\u001b[0;32m    243\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39msave \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39msave_txt:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\hand_\\lib\\site-packages\\ultralytics\\engine\\predictor.py:215\u001b[0m, in \u001b[0;36mBasePredictor.setup_source\u001b[1;34m(self, source)\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimgsz \u001b[38;5;241m=\u001b[39m check_imgsz(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mimgsz, stride\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mstride, min_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# check image size\u001b[39;00m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransforms\u001b[39m\u001b[38;5;124m'\u001b[39m, classify_transforms(\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimgsz[\u001b[38;5;241m0\u001b[39m])) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtask \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclassify\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 215\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset \u001b[38;5;241m=\u001b[39m \u001b[43mload_inference_source\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    216\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mimgsz\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimgsz\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    217\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mvid_stride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvid_stride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    218\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream_buffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    219\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msource_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39msource_type\n\u001b[0;32m    220\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m  \u001b[38;5;66;03m# streams\u001b[39;00m\n\u001b[0;32m    221\u001b[0m                                           \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1000\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m  \u001b[38;5;66;03m# images\u001b[39;00m\n\u001b[0;32m    222\u001b[0m                                           \u001b[38;5;28many\u001b[39m(\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvideo_flag\u001b[39m\u001b[38;5;124m'\u001b[39m, [\u001b[38;5;28;01mFalse\u001b[39;00m]))):  \u001b[38;5;66;03m# videos\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\hand_\\lib\\site-packages\\ultralytics\\data\\build.py:166\u001b[0m, in \u001b[0;36mload_inference_source\u001b[1;34m(source, imgsz, vid_stride, buffer)\u001b[0m\n\u001b[0;32m    164\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m source\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m webcam:\n\u001b[1;32m--> 166\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[43mLoadStreams\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimgsz\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimgsz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvid_stride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvid_stride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m screenshot:\n\u001b[0;32m    168\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m LoadScreenshots(source, imgsz\u001b[38;5;241m=\u001b[39mimgsz)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\hand_\\lib\\site-packages\\ultralytics\\data\\loaders.py:92\u001b[0m, in \u001b[0;36mLoadStreams.__init__\u001b[1;34m(self, sources, imgsz, vid_stride, buffer)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcaps[i] \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mVideoCapture(s)  \u001b[38;5;66;03m# store video capture object\u001b[39;00m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcaps[i]\u001b[38;5;241m.\u001b[39misOpened():\n\u001b[1;32m---> 92\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mst\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mFailed to open \u001b[39m\u001b[38;5;132;01m{\u001b[39;00ms\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     93\u001b[0m w \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcaps[i]\u001b[38;5;241m.\u001b[39mget(cv2\u001b[38;5;241m.\u001b[39mCAP_PROP_FRAME_WIDTH))\n\u001b[0;32m     94\u001b[0m h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcaps[i]\u001b[38;5;241m.\u001b[39mget(cv2\u001b[38;5;241m.\u001b[39mCAP_PROP_FRAME_HEIGHT))\n",
      "\u001b[1;31mConnectionError\u001b[0m: 1/1: 0... Failed to open 0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ⚠️ Video stream unresponsive, please check your IP camera connection.\n",
      "WARNING ⚠️ Video stream unresponsive, please check your IP camera connection.\n",
      "WARNING ⚠️ Video stream unresponsive, please check your IP camera connection.\n",
      "WARNING ⚠️ Video stream unresponsive, please check your IP camera connection.\n",
      "WARNING ⚠️ Video stream unresponsive, please check your IP camera connection.\n",
      "WARNING ⚠️ Video stream unresponsive, please check your IP camera connection.\n",
      "WARNING ⚠️ Video stream unresponsive, please check your IP camera connection.\n",
      "WARNING ⚠️ Video stream unresponsive, please check your IP camera connection.\n",
      "WARNING ⚠️ Video stream unresponsive, please check your IP camera connection.\n",
      "WARNING ⚠️ Video stream unresponsive, please check your IP camera connection.\n",
      "WARNING ⚠️ Video stream unresponsive, please check your IP camera connection.\n",
      "WARNING ⚠️ Video stream unresponsive, please check your IP camera connection.\n",
      "WARNING ⚠️ Video stream unresponsive, please check your IP camera connection.\n",
      "WARNING ⚠️ Video stream unresponsive, please check your IP camera connection.\n",
      "WARNING ⚠️ Video stream unresponsive, please check your IP camera connection.\n",
      "WARNING ⚠️ Video stream unresponsive, please check your IP camera connection.\n",
      "WARNING ⚠️ Video stream unresponsive, please check your IP camera connection.\n",
      "WARNING ⚠️ Video stream unresponsive, please check your IP camera connection.\n",
      "WARNING ⚠️ Video stream unresponsive, please check your IP camera connection.\n",
      "WARNING ⚠️ Video stream unresponsive, please check your IP camera connection.\n",
      "WARNING ⚠️ Video stream unresponsive, please check your IP camera connection.\n",
      "WARNING ⚠️ Video stream unresponsive, please check your IP camera connection.\n",
      "WARNING ⚠️ Video stream unresponsive, please check your IP camera connection.\n",
      "WARNING ⚠️ Video stream unresponsive, please check your IP camera connection.\n",
      "WARNING ⚠️ Video stream unresponsive, please check your IP camera connection.\n",
      "WARNING ⚠️ Video stream unresponsive, please check your IP camera connection.\n",
      "WARNING ⚠️ Video stream unresponsive, please check your IP camera connection.\n",
      "WARNING ⚠️ Video stream unresponsive, please check your IP camera connection.\n",
      "WARNING ⚠️ Video stream unresponsive, please check your IP camera connection.\n",
      "WARNING ⚠️ Video stream unresponsive, please check your IP camera connection.\n",
      "WARNING ⚠️ Video stream unresponsive, please check your IP camera connection.\n",
      "WARNING ⚠️ Video stream unresponsive, please check your IP camera connection.\n",
      "WARNING ⚠️ Video stream unresponsive, please check your IP camera connection.\n",
      "WARNING ⚠️ Video stream unresponsive, please check your IP camera connection.\n",
      "WARNING ⚠️ Video stream unresponsive, please check your IP camera connection.\n",
      "WARNING ⚠️ Video stream unresponsive, please check your IP camera connection.\n",
      "WARNING ⚠️ Video stream unresponsive, please check your IP camera connection.\n",
      "WARNING ⚠️ Video stream unresponsive, please check your IP camera connection.\n",
      "WARNING ⚠️ Video stream unresponsive, please check your IP camera connection.\n",
      "WARNING ⚠️ Video stream unresponsive, please check your IP camera connection.\n",
      "WARNING ⚠️ Video stream unresponsive, please check your IP camera connection.\n",
      "WARNING ⚠️ Video stream unresponsive, please check your IP camera connection.\n",
      "WARNING ⚠️ Video stream unresponsive, please check your IP camera connection.\n",
      "WARNING ⚠️ Video stream unresponsive, please check your IP camera connection.\n",
      "WARNING ⚠️ Video stream unresponsive, please check your IP camera connection.\n",
      "WARNING ⚠️ Video stream unresponsive, please check your IP camera connection.\n"
     ]
    }
   ],
   "source": [
    "model_directory = r'signLenguage_Model.pt'\n",
    "labels = {0: 'A', 1: 'B', 2: 'C', 3: 'D', 4: 'F', 5: 'Rock', 6: 'U', 7: 'V', 8:'Y'}\n",
    "source = '0'\n",
    "implementation(model_directory, labels, source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41cc3ef-4ab2-4923-b68d-338cd7ab06b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "467f3d91-a5e2-4d87-968d-1870bc7a020d",
   "metadata": {},
   "source": [
    "## Usage: Body detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d189700d-3456-4e62-9d05-9c9f82767cf5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "# Inicializa el módulo MediaPipe Pose\n",
    "mp_pose = mp.solutions.pose\n",
    "\n",
    "# Inicializa la cámara\n",
    "cap = cv2.VideoCapture(1)\n",
    "\n",
    "# Inicializa el detector de pose\n",
    "with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            continue\n",
    "\n",
    "        # Convierte la imagen a blanco y negro\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Realiza la detección de la pose\n",
    "        results = pose.process(frame_rgb)\n",
    "\n",
    "        if results.pose_landmarks:\n",
    "            # Dibuja las landmarks y líneas en la imagen\n",
    "            mp_drawing = mp.solutions.drawing_utils\n",
    "            mp_drawing.draw_landmarks(frame, results.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
    "\n",
    "            # Obtiene las coordenadas del bounding box\n",
    "            bbox_cords = []\n",
    "            for landmark in results.pose_landmarks.landmark:\n",
    "                h, w, _ = frame.shape\n",
    "                cx, cy = int(landmark.x * w), int(landmark.y * h)\n",
    "                bbox_cords.append((cx, cy))\n",
    "            min_x = min(bbox_cords, key=lambda item: item[0])[0]\n",
    "            min_y = min(bbox_cords, key=lambda item: item[1])[1]\n",
    "            max_x = max(bbox_cords, key=lambda item: item[0])[0]\n",
    "            max_y = max(bbox_cords, key=lambda item: item[1])[1]\n",
    "\n",
    "            # Dibuja el bounding box\n",
    "            cv2.rectangle(frame, (min_x, min_y), (max_x, max_y), (0, 255, 0), 2)\n",
    "\n",
    "        cv2.putText(frame, f'Saluda! Te estamos grabando :)',\n",
    "                    (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (50, 50, 255), 3)\n",
    "\n",
    "        width, height = 1600, 900\n",
    "        frame = cv2.resize(frame, (width, height))\n",
    "        cv2.imshow('Hand Landmarks', frame)\n",
    "        \n",
    "        # Si se presiona la tecla 'q', se sale del bucle\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "# Libera la cámara y cierra las ventanas\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
