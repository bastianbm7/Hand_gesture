{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3466658-3356-4126-b403-e44d4d57cc0d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Consulta a ChatGPT usando tu mano y tu micrófono"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922c2d86-3ae5-4ca8-adc8-1c2f01bd3310",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Al correr el código, se abrirá una la cámara y podrás cambiar distintas funciones:**\n",
    "\n",
    "1) Cambia el valor de 'reconocer_manos' para que reconocer las manos en el video.\n",
    "\n",
    "2) Cambia el valor de 'usar_numeros' para mostrar las 21 marcas en la mano.\n",
    "\n",
    "3) Usa 'cambiar_color' para cambiar el color de algunas marcas de la mano. Se debe ingresar como lista con valores entre 0 a 20, por ejemplo; \\[2, 5, 8, 17\\]\n",
    "\n",
    "4) Cambia 'color' para cambiar el color de las marcas señaladas. Los valores son en escala BGR (Azul, Verde, Rojo) desde 0 a 255.\n",
    "\n",
    "5) Usa 'pregunta' para realizar una pregunta a chatGPT cuando tengas las manos cerca de la pantalla. Una vez realizada la pregunta, vuelve a alejarte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "31167566-7700-4859-9bd0-fd6829d6fdb8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ----------------------------------------------\n",
    "# Read libraries\n",
    "# ----------------------------------------------\n",
    "import cv2\n",
    "import time\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import openai\n",
    "import speech_recognition as sr\n",
    "import mediapipe as mp\n",
    "import pyttsx3\n",
    "\n",
    "from ultralytics import YOLO\n",
    "from ultralytics.models.yolo.detect.predict import DetectionPredictor\n",
    "\n",
    "import Codes\n",
    "\n",
    "# ----------------------------------------------\n",
    "# Inicializadores\n",
    "# ----------------------------------------------\n",
    "\n",
    "# Configura tu clave de API de OpenAI aquí\n",
    "openai.api_key = \"YOUR API KEY\"\n",
    "# Inicializa el reconocedor de voz\n",
    "recognizer = sr.Recognizer()\n",
    "# Initialize MediaPipe Hands\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands()\n",
    "# Initialize MediaPipe Drawing\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Inicializar el sintetizador de voz\n",
    "engine = pyttsx3.init()\n",
    "engine.setProperty('rate', 180)\n",
    "\n",
    "# ----------------------------------------------\n",
    "# Hand recognizer implemented with chat GPT\n",
    "# ----------------------------------------------\n",
    "def handTracking(show, number, changeColor, color, question):\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            continue\n",
    "        # Convert the BGR image to RGB\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        # Process the frame to detect hands\n",
    "        results = hands.process(frame_rgb)\n",
    "    \n",
    "        if results.multi_hand_landmarks:\n",
    "            # Determinar el centro de la imagen\n",
    "            center_x = frame.shape[1] // 2\n",
    "            \n",
    "            for i, landmarks in enumerate(results.multi_hand_landmarks):\n",
    "                # Calcular la posición horizontal del centro de la mano\n",
    "                x_min = min(landmark.x for landmark in landmarks.landmark)\n",
    "                x_max = max(landmark.x for landmark in landmarks.landmark)\n",
    "                hand_center_x = (x_min + x_max) / 2 * frame.shape[1]\n",
    "                \n",
    "                # Determinar la mano izquierda o derecha\n",
    "                if hand_center_x < center_x:\n",
    "                    hand_type = \"Mano derecha\"\n",
    "                else:\n",
    "                    hand_type = \"Mano Izquierda\"\n",
    "                \n",
    "                # Draw landmarks on the frame\n",
    "                mp_drawing.draw_landmarks(frame, landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "    \n",
    "                # Obtener las coordenadas del bounding box\n",
    "                x_min = min(landmark.x for landmark in landmarks.landmark)\n",
    "                y_min = min(landmark.y for landmark in landmarks.landmark)\n",
    "                x_max = max(landmark.x for landmark in landmarks.landmark)\n",
    "                y_max = max(landmark.y for landmark in landmarks.landmark)\n",
    "                \n",
    "                # Dibujar el cuadro alrededor de la mano\n",
    "                cv2.rectangle(frame, (int(x_min * frame.shape[1]), int(y_min * frame.shape[0])),\n",
    "                              (int(x_max * frame.shape[1]), int(y_max * frame.shape[0])),\n",
    "                              (0, 255, 0), 2)\n",
    "                \n",
    "                # Agregar texto con el tipo de mano\n",
    "                cv2.putText(frame, hand_type, (int(x_min * frame.shape[1]), int(y_min * frame.shape[0]) - 10),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)\n",
    "    \n",
    "                if question:\n",
    "                    # Calcular el área del rectángulo que contiene la mano\n",
    "                    area = (x_max - x_min) * (y_max - y_min) * 1000\n",
    "                \n",
    "                    # Si el área del rectángulo es mayor que un valor umbral, reconoce el discurso\n",
    "                    if area > 170:  # Ajusta este valor según tu necesidad\n",
    "                        with sr.Microphone() as source:\n",
    "                            # Indica las instrucciones con el sintetizador de voz\n",
    "                            engine.say('Has activado Chat GPT. Realiza una breve pregunta')\n",
    "                            engine.runAndWait()\n",
    "                            \n",
    "                            # Reconocer voz a texto\n",
    "                            recognizer.adjust_for_ambient_noise(source)\n",
    "                            audio = recognizer.listen(source)\n",
    "                            \n",
    "                            # Indica instrucciones con sintetizador de voz\n",
    "                            engine.say('Procesando tu pregunta')\n",
    "                            engine.runAndWait()\n",
    "                        \n",
    "                        # Intenta entender la pregunta\n",
    "                        try:\n",
    "                            pregunta = recognizer.recognize_google(audio, language=\"es-ES\")\n",
    "                        except sr.UnknownValueError:\n",
    "                            print(\"No se pudo entender el audio\")\n",
    "                        except sr.RequestError as e:\n",
    "                            print(\"Error en la solicitud: {0}\".format(e))\n",
    "                        \n",
    "                        # Si se entiende la pregunta, se ingresa como input a chatGPT\n",
    "                        if pregunta:\n",
    "                            respuesta = openai.Completion.create(\n",
    "                                engine=\"text-davinci-002\",\n",
    "                                prompt=pregunta,\n",
    "                                max_tokens=200  # Ajusta el número de tokens según tu preferencia\n",
    "                            )\n",
    "                            print(\"Pregunta:\", pregunta)\n",
    "                            \n",
    "                            # Guardar respuesta\n",
    "                            respuesta_texto = respuesta.choices[0].text\n",
    "                            \n",
    "                            # Usamos el sintetizador de voz para reproducir la respuesta\n",
    "                            engine.say(respuesta_texto)\n",
    "                            engine.runAndWait()\n",
    "                            time.sleep(1)\n",
    "                            \n",
    "                            # Realiza instrucciones con el sintetizador de voz\n",
    "                            engine.say('Gracias. Ahora puedes volver a alejar tu mano')\n",
    "                            engine.runAndWait()\n",
    "                            time.sleep(1)\n",
    "                    \n",
    "                if number == True:\n",
    "                    # Add numbers to landmarks\n",
    "                    for idx, landmark in enumerate(landmarks.landmark):\n",
    "                        h, w, _ = frame.shape\n",
    "                        cx, cy = int(landmark.x * w), int(landmark.y * h)\n",
    "                        cv2.putText(frame, str(idx), (cx, cy), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n",
    "                \n",
    "                if changeColor is not None:\n",
    "                    for mark in changeColor:\n",
    "                        # Change the color of a specific landmark (e.g., landmark 8)\n",
    "                        landmark_index_to_color = mark\n",
    "                        landmark_to_change = landmarks.landmark[landmark_index_to_color]\n",
    "                        cx, cy = int(landmark_to_change.x * frame.shape[1]), int(landmark_to_change.y * frame.shape[0])\n",
    "                        cv2.circle(frame, (cx, cy), 5, color, -1)  # Change color to red (BGR format)\n",
    "        \n",
    "        # Write a instructions on screen\n",
    "        if question:\n",
    "            cv2.putText(frame, f'Acerca tu mano para realizar una consulta a chatGPT',\n",
    "                        (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 150, 255), 2)\n",
    "        \n",
    "        cv2.putText(frame, f'Saluda! Te estamos grabando :)',\n",
    "                    (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (50, 50, 255), 3)\n",
    "        \n",
    "        width, height = 1600, 900\n",
    "        frame = cv2.resize(frame, (width, height))\n",
    "        cv2.imshow('Hand Landmarks', frame)\n",
    "        \n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# ----------------------------------------------\n",
    "# Use sign language recognizer from roboflow\n",
    "# ----------------------------------------------\n",
    "def implementation(model_directory, labels, source):\n",
    "    model = YOLO(model_directory)\n",
    "\n",
    "    model.predict(source=source, show=True)  # Generador de objetos Results\n",
    "    results = model(source=source, stream=True, show = True)  # Generador de objetos Results\n",
    "    \n",
    "    for r in results:\n",
    "        boxes = r.boxes  # Objetos Boxes para las salidas de cuadros delimitadores\n",
    "        masks = r.masks  # Objetos Masks para las salidas de máscaras de segmentación\n",
    "        probs = r.probs  # Probabilidades de clase para las salidas de clasificación\n",
    "        \n",
    "        count = 0\n",
    "        if len(boxes) > 0:\n",
    "            classes = int(boxes.cls[0])\n",
    "            predCharacter = labels[classes]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8cdddd-9ad3-47c8-998c-753c3046c8f2",
   "metadata": {},
   "source": [
    "## Usage: Hand detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5637281a-e610-4009-a3ef-186c47fe49a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Open the webcam\n",
    "cap = cv2.VideoCapture(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a909de-5709-4091-94e2-1a7ea2394b96",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "show = True  # True / False\n",
    "number = False  # True / False\n",
    "changeColor = [8,17, 4, 16] # Lista con números del 0 al 20.\n",
    "color = (250, 155, 0)  # Colores BGR (Blue, Green, Red)\n",
    "question = False  # True / False\n",
    "\n",
    "handTracking(show, number, changeColor, color, question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba8d94e-181a-4d03-8ce9-5140e5599e37",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Usage: Hand Gesture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125db4b1-97a6-48f9-b75b-1540006dbede",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_directory = r'signLenguage_Model.pt'\n",
    "labels = {0: 'A', 1: 'B', 2: 'C', 3: 'D', 4: 'F', 5: 'Rock', 6: 'U', 7: 'V', 8:'Y'}\n",
    "source = '1'\n",
    "implementation(model_directory, labels, source)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467f3d91-a5e2-4d87-968d-1870bc7a020d",
   "metadata": {},
   "source": [
    "## Usage: Body detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d189700d-3456-4e62-9d05-9c9f82767cf5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "# Inicializa el módulo MediaPipe Pose\n",
    "mp_pose = mp.solutions.pose\n",
    "\n",
    "# Inicializa la cámara\n",
    "cap = cv2.VideoCapture(1)\n",
    "\n",
    "# Inicializa el detector de pose\n",
    "with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            continue\n",
    "\n",
    "        # Convierte la imagen a blanco y negro\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Realiza la detección de la pose\n",
    "        results = pose.process(frame_rgb)\n",
    "\n",
    "        if results.pose_landmarks:\n",
    "            # Dibuja las landmarks y líneas en la imagen\n",
    "            mp_drawing = mp.solutions.drawing_utils\n",
    "            mp_drawing.draw_landmarks(frame, results.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
    "\n",
    "            # Obtiene las coordenadas del bounding box\n",
    "            bbox_cords = []\n",
    "            for landmark in results.pose_landmarks.landmark:\n",
    "                h, w, _ = frame.shape\n",
    "                cx, cy = int(landmark.x * w), int(landmark.y * h)\n",
    "                bbox_cords.append((cx, cy))\n",
    "            min_x = min(bbox_cords, key=lambda item: item[0])[0]\n",
    "            min_y = min(bbox_cords, key=lambda item: item[1])[1]\n",
    "            max_x = max(bbox_cords, key=lambda item: item[0])[0]\n",
    "            max_y = max(bbox_cords, key=lambda item: item[1])[1]\n",
    "\n",
    "            # Dibuja el bounding box\n",
    "            cv2.rectangle(frame, (min_x, min_y), (max_x, max_y), (0, 255, 0), 2)\n",
    "\n",
    "        cv2.putText(frame, f'Saluda! Te estamos grabando :)',\n",
    "                    (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (50, 50, 255), 3)\n",
    "\n",
    "        width, height = 1600, 900\n",
    "        frame = cv2.resize(frame, (width, height))\n",
    "        cv2.imshow('Hand Landmarks', frame)\n",
    "        \n",
    "        # Si se presiona la tecla 'q', se sale del bucle\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "# Libera la cámara y cierra las ventanas\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
